{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "styleImage = Image.open('../data/style/cubism.jpg')\n",
    "contentImage = Image.open('../data/content/nalax.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512,512)),  # Resize the images to a suitable size\n",
    "    transforms.ToTensor(),  # Convert the images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])  # Normalize the images\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GramMatrix(nn.Module):\n",
    "    def forward(self, input):\n",
    "        b, c, h, w = input.size()\n",
    "        F = input.view(b, c, h * w)\n",
    "        G = torch.bmm(F, F.transpose(1, 2))\n",
    "        return G.div(h * w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        \n",
    "        vgg = models.vgg19(pretrained=True).features\n",
    "        self.slice1 = nn.Sequential(*list(vgg.children())[:2])\n",
    "        self.slice2 = nn.Sequential(*list(vgg.children())[2:7])\n",
    "        self.slice3 = nn.Sequential(*list(vgg.children())[7:12])\n",
    "        self.slice4 = nn.Sequential(*list(vgg.children())[12:21])\n",
    "        \n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        loss = nn.MSELoss()\n",
    "        input_slices = [self.slice1(input), self.slice2(input), self.slice3(input), self.slice4(input)]\n",
    "        target_slices = [self.slice1(target), self.slice2(target), self.slice3(target), self.slice4(target)]\n",
    "        losses = [loss(input_slice, target_slice) for input_slice, target_slice in zip(input_slices, target_slices)]\n",
    "        return sum(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransfer(nn.Module):\n",
    "    def __init__(self, style, content, alpha, beta, lr=0.01):\n",
    "        super(StyleTransfer, self).__init__()\n",
    "        \n",
    "        self.style = style\n",
    "        self.content = content\n",
    "        \n",
    "        self.content_layers = ['slice3']\n",
    "        self.style_layers = ['slice1', 'slice2', 'slice3', 'slice4']\n",
    "        self.content_weight = alpha\n",
    "        self.style_weight = beta\n",
    "        \n",
    "        self.loss_network = nn.Sequential()\n",
    "        self.gram = GramMatrix()\n",
    "        self.perceptual_loss = PerceptualLoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.loss_network.cuda()\n",
    "            self.gram.cuda()\n",
    "            self.perceptual_loss.cuda()\n",
    "            self.content = self.content.cuda()\n",
    "            self.style = self.style.cuda()\n",
    "        else:\n",
    "            self.loss_network.cpu()\n",
    "            self.gram.cpu()\n",
    "            self.perceptual_loss.cpu()\n",
    "            self.content = self.content.cpu()\n",
    "            self.style = self.style.cpu()\n",
    "\n",
    "    def train(self, steps):\n",
    "        for i in range(steps):\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            pastiche = self.content.clone().requires_grad_()\n",
    "            \n",
    "            content_features = {}\n",
    "            style_features = {}\n",
    "            output_features = {}\n",
    "            for name, module in self.loss_network._modules.items():\n",
    "                pastiche = module(pastiche)\n",
    "                \n",
    "                if name in self.content_layers:\n",
    "                    content_features[name] = pastiche\n",
    "                \n",
    "                if name in self.style_layers:\n",
    "                    style_features[name] = self.gram(pastiche)\n",
    "                    \n",
    "                output_features[name] = pastiche\n",
    "            \n",
    "            content_loss = self.perceptual_loss(content_features['slice3'], self.content)\n",
    "            style_loss = sum([self.perceptual_loss(self.gram(output_features[name]), style_features[name]) for name in style_features])\n",
    "            \n",
    "            total_loss = self.content_weight * content_loss + self.style_weight * style_loss\n",
    "            total_loss.backward()\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "        return pastiche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1 # Content weight\n",
    "beta = 1000 # Style weight\n",
    "num_steps = 2000 # Number of iterations to run the optimization\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_tensor = transform(styleImage).unsqueeze(0)\n",
    "content_tensor = transform(contentImage).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = StyleTransfer(style_tensor, content_tensor, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'slice3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m canva_tensor \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain(num_steps)\n",
      "Cell \u001b[0;32mIn[5], line 52\u001b[0m, in \u001b[0;36mStyleTransfer.train\u001b[0;34m(self, steps)\u001b[0m\n\u001b[1;32m     48\u001b[0m         style_features[name] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgram(pastiche)\n\u001b[1;32m     50\u001b[0m     output_features[name] \u001b[39m=\u001b[39m pastiche\n\u001b[0;32m---> 52\u001b[0m content_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mperceptual_loss(content_features[\u001b[39m'\u001b[39;49m\u001b[39mslice3\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontent)\n\u001b[1;32m     53\u001b[0m style_loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mperceptual_loss(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgram(output_features[name]), style_features[name]) \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m style_features])\n\u001b[1;32m     55\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontent_weight \u001b[39m*\u001b[39m content_loss \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstyle_weight \u001b[39m*\u001b[39m style_loss\n",
      "\u001b[0;31mKeyError\u001b[0m: 'slice3'"
     ]
    }
   ],
   "source": [
    "canva_tensor = model.train(num_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
