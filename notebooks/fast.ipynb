{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastStyleTransfer:\n",
    "    def __init__(self, content_image_path, style_image_path, output_image_path, content_weight=1, style_weight=1000,\n",
    "                 num_iterations=1000, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "        self.content_image_path = content_image_path\n",
    "        self.style_image_path = style_image_path\n",
    "        self.output_image_path = output_image_path\n",
    "        self.content_weight = content_weight\n",
    "        self.style_weight = style_weight\n",
    "        self.num_iterations = num_iterations\n",
    "        self.device = device\n",
    "\n",
    "        # Load the VGG19 model and set it to evaluation mode\n",
    "        self.vgg = models.vgg19(pretrained=True).features.to(self.device)\n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "    def load_image(self, image_path):\n",
    "        image = Image.open(image_path)\n",
    "        image = transforms.ToTensor()(image)\n",
    "        image = image.unsqueeze(0).to(self.device)\n",
    "        return image\n",
    "\n",
    "    def save_image(self, image, image_path):\n",
    "        image = image.cpu().clone()\n",
    "        image = image.squeeze(0)\n",
    "        image = transforms.ToPILImage()(image)\n",
    "        image.save(image_path)\n",
    "\n",
    "    def gram_matrix(self, input):\n",
    "        b, c, h, w = input.size(0), input.size(1), input.size(2), input.size(3)  # Update here\n",
    "        features = input.view(b * c, h * w)\n",
    "        gram_matrix = torch.mm(features, features.t())\n",
    "        return gram_matrix\n",
    "\n",
    "\n",
    "    def compute_content_loss(self, content_features, output_features):\n",
    "        content_loss = F.mse_loss(output_features, content_features)\n",
    "        return content_loss\n",
    "\n",
    "    def compute_style_loss(self, style_features, output_features):\n",
    "        style_loss = 0.0\n",
    "        for ft_y, gm_s in zip(output_features, style_features):\n",
    "            gm_y = self.gram_matrix(ft_y)\n",
    "            style_loss += F.mse_loss(gm_y, gm_s)\n",
    "        return style_loss\n",
    "\n",
    "    def build_model(self, content, style):\n",
    "        target = content.clone().requires_grad_(True)\n",
    "        optimizer = optim.LBFGS([target])\n",
    "        content_features = self.vgg(content)\n",
    "        style_features = self.vgg(style)\n",
    "        style_grams = [self.gram_matrix(y) for y in style_features]\n",
    "        return target, optimizer, content_features, style_features, style_grams\n",
    "\n",
    "    def run(self):\n",
    "    # Load content and style images\n",
    "        content = self.load_image(self.content_image_path)\n",
    "        style = self.load_image(self.style_image_path)\n",
    "\n",
    "        # Build the model\n",
    "        target, optimizer, content_features, style_features, style_grams = self.build_model(content, style)\n",
    "\n",
    "        # Run the optimization loop\n",
    "        print(\"Running style transfer...\")\n",
    "        step = 0\n",
    "        while step <= self.num_iterations:\n",
    "            def closure():\n",
    "                nonlocal step\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                output = self.vgg(target)\n",
    "                content_loss = self.compute_content_loss(content_features, output)\n",
    "                style_loss = self.compute_style_loss(style_features, output)\n",
    "\n",
    "                total_loss = self.content_weight * content_loss + self.style_weight * style_loss\n",
    "                total_loss.backward()\n",
    "\n",
    "                step += 1\n",
    "                if step % 100 == 0:\n",
    "                    print(f'Step [{step}/{self.num_iterations}], Loss: {total_loss.item()}')\n",
    "\n",
    "                return total_loss\n",
    "\n",
    "            optimizer.step(closure)\n",
    "\n",
    "        # Save the output image\n",
    "        self.save_image(target, self.output_image_path)\n",
    "        print(f'Style transfer complete. Output image saved at {self.output_image_path}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\david\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m output_image_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m../data/content/adri.jpeg\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      6\u001b[0m fast_style_transfer \u001b[39m=\u001b[39m FastStyleTransfer(content_image_path, style_image_path, output_image_path)\n\u001b[1;32m----> 7\u001b[0m fast_style_transfer\u001b[39m.\u001b[39;49mrun()\n",
      "Cell \u001b[1;32mIn[7], line 60\u001b[0m, in \u001b[0;36mFastStyleTransfer.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     57\u001b[0m style \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_image(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstyle_image_path)\n\u001b[0;32m     59\u001b[0m \u001b[39m# Build the model\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m target, optimizer, content_features, style_features, style_grams \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_model(content, style)\n\u001b[0;32m     62\u001b[0m \u001b[39m# Run the optimization loop\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRunning style transfer...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 51\u001b[0m, in \u001b[0;36mFastStyleTransfer.build_model\u001b[1;34m(self, content, style)\u001b[0m\n\u001b[0;32m     49\u001b[0m content_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvgg(content)\n\u001b[0;32m     50\u001b[0m style_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvgg(style)\n\u001b[1;32m---> 51\u001b[0m style_grams \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgram_matrix(y) \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m style_features]\n\u001b[0;32m     52\u001b[0m \u001b[39mreturn\u001b[39;00m target, optimizer, content_features, style_features, style_grams\n",
      "Cell \u001b[1;32mIn[7], line 51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m content_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvgg(content)\n\u001b[0;32m     50\u001b[0m style_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvgg(style)\n\u001b[1;32m---> 51\u001b[0m style_grams \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgram_matrix(y) \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m style_features]\n\u001b[0;32m     52\u001b[0m \u001b[39mreturn\u001b[39;00m target, optimizer, content_features, style_features, style_grams\n",
      "Cell \u001b[1;32mIn[7], line 30\u001b[0m, in \u001b[0;36mFastStyleTransfer.gram_matrix\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgram_matrix\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m---> 30\u001b[0m     b, c, h, w \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()\n\u001b[0;32m     31\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mview(b \u001b[39m*\u001b[39m c, h \u001b[39m*\u001b[39m w)\n\u001b[0;32m     32\u001b[0m     gram_matrix \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmm(features, features\u001b[39m.\u001b[39mt())\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "source": [
    "content_image_path = '../data/content/adri.jpeg'\n",
    "style_image_path = '../data/style/rain_princess.jpg'\n",
    "output_image_path = '../data/content/adri.jpeg'\n",
    "\n",
    "\n",
    "fast_style_transfer = FastStyleTransfer(content_image_path, style_image_path, output_image_path)\n",
    "fast_style_transfer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
