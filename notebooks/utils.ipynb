{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message=\"The parameter 'pretrained' is deprecated\")\n",
    "warnings.filterwarnings('ignore', message=\"Arguments other than a weight enum or `None` for 'weights' are deprecated\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision.utils import save_image\n",
    "import scipy.misc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imsize = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = transforms.Compose([\n",
    "            transforms.Resize(imsize),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "unloader = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImage(name):\n",
    "    image = Image.open(name)\n",
    "    image = Variable(loader(image))\n",
    "    image = image.unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "def saveImage(input, path):\n",
    "    image = input.data.clone().cpu()\n",
    "    batch_size = image.size(0)\n",
    "    image = image.view(batch_size, 3, imsize, imsize)\n",
    "    image = unloader(image)\n",
    "    scipy.misc.imsave(path, image)\n",
    "    \n",
    "def im_convert(tensor):\n",
    "    ''' Presentar imagen como Tensor'''\n",
    "    imagen = tensor.to('cpu').clone().detach()\n",
    "    imagen = imagen.numpy().squeeze()\n",
    "    imagen = imagen.transpose(1, 2, 0)\n",
    "    #imagen = imagen * np.array((0.029, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    imagen = imagen.clip(0, 1)\n",
    "    \n",
    "    return imagen\n",
    "\n",
    "def showImages(content, style, output):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(20, 10))\n",
    "\n",
    "    ax1.imshow(im_convert(content))\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Content Image')\n",
    "\n",
    "    ax2.imshow(im_convert(style))\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('Style Image')\n",
    "\n",
    "    ax3.imshow(im_convert(output))\n",
    "    ax3.axis('off')\n",
    "    ax3.set_title('Merged Image')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GramMatrix(nn.Module):\n",
    "\n",
    "    def forward(self, input):\n",
    "        a,b,c,d = input.size()\n",
    "        features = input.view(a*b, c*d)\n",
    "        G = torch.mm(features, features.t())\n",
    "        return G.div(a*b*c*d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(object):\n",
    "    def __init__(self, style, content, pastiche):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.style = style\n",
    "        self.content = content\n",
    "        self.pastiche = nn.Parameter(pastiche.data)\n",
    "\n",
    "        self.content_layers = ['conv_4']\n",
    "        self.style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "        self.contentWeight = 1\n",
    "        self.styleWeight = 1000\n",
    "\n",
    "        self.loss_network = models.vgg19(pretrained=True)\n",
    "\n",
    "        self.gram = GramMatrix()\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.LBFGS([self.pastiche])\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.loss_network.cuda()\n",
    "            self.gram.cuda()\n",
    "        else:\n",
    "            self.loss_network.cpu()\n",
    "            self.gram.cpu()\n",
    "\n",
    "    def train(self):\n",
    "        def closure():\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            pastiche = self.pastiche.clone()\n",
    "            pastiche.data.clamp_(0,1)\n",
    "            content = self.content.clone()\n",
    "            style = self.style.clone()\n",
    "\n",
    "            content_loss = 0\n",
    "            style_loss = 0\n",
    "\n",
    "            i = 1\n",
    "            not_inplace = lambda layer: nn.ReLU(inplace=False) if isinstance(layer, nn.ReLU) else layer\n",
    "            for layer in list(self.loss_network.features):\n",
    "                layer = not_inplace(layer)\n",
    "                if self.use_cuda:\n",
    "                    layer.cuda()\n",
    "                else:\n",
    "                    layer.cpu()\n",
    "                \n",
    "                pastiche, content, style = layer.forward(pastiche), layer.forward(content), layer.forward(style)\n",
    "\n",
    "                if isinstance(layer, nn.Conv2d):\n",
    "                    name = \"conv_\" +str(i)\n",
    "\n",
    "                    if name in self.content_layers:\n",
    "                        content_loss += self.loss(pastiche * self.contentWeight, content.detach() * self.contentWeight)\n",
    "\n",
    "                    if name in self.style_layers:\n",
    "                        pastiche_g, style_g = self.gram.forward(pastiche), self.gram.forward(style)\n",
    "                        style_loss += self.loss(pastiche_g * self.styleWeight, style_g.detach() * self.styleWeight)\n",
    "                    \n",
    "                if isinstance(layer, nn.ReLU):\n",
    "                    i += 1\n",
    "            \n",
    "            total_loss = content_loss + style_loss\n",
    "            total_loss.backward()\n",
    "\n",
    "            return total_loss\n",
    "    \n",
    "        self.optimizer.step(closure)\n",
    "        return self.pastiche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = loadImage('../data/style/composition.jpg')\n",
    "content = loadImage('../data/content/nalax.jpg')\n",
    "\n",
    "pastiche = loadImage('../data/content/nalax.jpg').type(dtype)\n",
    "# This is to make random noise image (randomize pixels)\n",
    "#pastiche.data = torch.randn(pastiche.data.size()).type(dtype)\n",
    "\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize = (20, 10))\n",
    "ax1.imshow(im_convert(content))\n",
    "ax2.imshow(im_convert(style))\n",
    "ax3.imshow(im_convert(pastiche))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN(style, content, pastiche)\n",
    "for i in range(epochs):\n",
    "    pastiche = cnn.train()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(\"Iteration: %d\" % (i))\n",
    "\n",
    "        path = '../data/out/output/%d.png' % (i)\n",
    "        pastiche.data.clamp_(0 ,1)\n",
    "        #print(\"PÃ©rdida Total: \", cnn.)\n",
    "        plt.imshow(im_convert(pastiche))\n",
    "        plt.show()\n",
    "        \n",
    "    if i == epochs:        \n",
    "        save_image(pastiche, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showImages(content,style,pastiche)"
   ]
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "version": 1
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
