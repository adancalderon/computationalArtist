{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFM(object):\n",
    "    def __init__(self, style, content, pastiche, alpha, beta):\n",
    "        super(FFM, self).__init__()\n",
    "        \n",
    "        self.style = style\n",
    "        self.content = content\n",
    "        self.pastiche = nn.Parameter(pastiche.data)\n",
    "\n",
    "        self.content_layers = ['conv_4']\n",
    "        self.style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "        self.contentWeight = alpha\n",
    "        self.styleWeight = beta\n",
    "\n",
    "        self.transform_network = nn.Sequential(nn.ReflectionPad2d(40),\n",
    "                                           nn.Conv2d(3, 32, 9, stride=1, padding=4),\n",
    "                                           nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "                                           nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "                                           nn.Conv2d(128, 128, 3, stride=1, padding=0),\n",
    "                                           nn.Conv2d(128, 128, 3, stride=1, padding=0),\n",
    "                                           nn.Conv2d(128, 128, 3, stride=1, padding=0),\n",
    "                                           nn.Conv2d(128, 128, 3, stride=1, padding=0),\n",
    "                                           nn.Conv2d(128, 128, 3, stride=1, padding=0),\n",
    "                                           nn.Conv2d(128, 128, 3, stride=1, padding=0),\n",
    "                                           nn.Conv2d(128, 128, 3, stride=1, padding=0),\n",
    "                                           nn.Conv2d(128, 128, 3, stride=1, padding=0),\n",
    "                                           nn.Conv2d(128, 128, 3, stride=1, padding=0),\n",
    "                                           nn.Conv2d(128, 128, 3, stride=1, padding=0),\n",
    "                                           nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "                                           nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "                                           nn.Conv2d(32, 3, 9, stride=1, padding=4),\n",
    "                                           )\n",
    "        self.gram = GramMatrix()\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.transform_network.parameters(), lr=1e-3)\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.loss_network.cuda()\n",
    "            self.gram.cuda()\n",
    "        else:\n",
    "            self.loss_network.cpu()\n",
    "            self.gram.cpu()\n",
    "\n",
    "    def train(self,content):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        content = content.clone()\n",
    "        style = self.style.clone()\n",
    "        pastiche = self.transformation_network.forward(content)\n",
    "\n",
    "        content_loss = 0\n",
    "        style_loss = 0\n",
    "\n",
    "        i = 1\n",
    "        not_inplace = lambda layer: nn.ReLU(inplace=False) if isinstance(layer, nn.ReLU) else layer\n",
    "        for layer in list(self.loss_network.features):\n",
    "            layer = not_inplace(layer)\n",
    "            if self.use_cuda:\n",
    "                layer.cuda()\n",
    "\n",
    "            pastiche, content, style = layer.forward(pastiche), layer.forward(content), layer.forward(style)\n",
    "\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                name = \"conv_\" + str(i)\n",
    "\n",
    "                if name in self.content_layers:\n",
    "                    content_loss += self.loss(pastiche * self.content_weight, content.detach() * self.content_weight)\n",
    "                if name in self.style_layers:\n",
    "                    pastiche_g, style_g = self.gram.forward(pastiche), self.gram.forward(style)\n",
    "                    style_loss += self.loss(pastiche_g * self.style_weight, style_g.detach() * self.style_weight)\n",
    "\n",
    "            if isinstance(layer, nn.ReLU):\n",
    "                i += 1\n",
    "\n",
    "        total_loss = content_loss + style_loss\n",
    "        total_loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return self.pastiche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PENDING\n",
    "num_epochs = 3\n",
    "N = 4\n",
    "\n",
    "def main():\n",
    "    style_cnn = FFM(style)\n",
    "\n",
    "    # Contents\n",
    "    wikiart = datasets.ImageFolder(root='path/to/wikiart', transform=loader)\n",
    "    content_loader = torch.utils.data.DataLoader(wikiart, batch_size=N, shuffle=True, **kwargs)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, content_batch in enumerate(content_loader):\n",
    "          iteration = epoch * i + i\n",
    "          content_loss, style_loss, pastiches = style_cnn.train(content_batch, style_batch)\n",
    "\n",
    "          if i % 10 == 0:\n",
    "              print(\"Iteration: %d\" % (iteration))\n",
    "              print(\"Content loss: %f\" % (content_loss.data[0]))\n",
    "              print(\"Style loss: %f\" % (style_loss.data[0]))\n",
    "\n",
    "          if i % 500 == 0:\n",
    "              path = \"outputs/%d_\" % (iteration)\n",
    "              paths = [path + str(n) + \".png\" for n in range(N)]\n",
    "              save_images(pastiches, paths)\n",
    "\n",
    "              path = \"outputs/content_%d_\" % (iteration)\n",
    "              paths = [path + str(n) + \".png\" for n in range(N)]\n",
    "              save_images(content_batch, paths)\n",
    "              style_cnn.save()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
