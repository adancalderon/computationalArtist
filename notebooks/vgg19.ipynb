{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "u8pmTiWayLlMonIE7ddxyN",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "ApWK5Hvz5u9BTytFQXe7Ub",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU(inplace=True)\n",
       "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (24): ReLU(inplace=True)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): ReLU(inplace=True)\n",
       "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (31): ReLU(inplace=True)\n",
       "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (33): ReLU(inplace=True)\n",
       "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (35): ReLU(inplace=True)\n",
       "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vgg = models.vgg19(pretrained = True).features\n",
    "\n",
    "# Congelar todos los parámetros del VGG19 por que solo estamos optimizando para una sola imagen\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "vgg.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "P7KchVIyvApVB2p6TRPuLl",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def cargar_imagen(directorio_imagen, tamaño_maximo = 500, shape = None ):\n",
    "    '''Cargar y transformar una imagen aegurando que su tamanño sea menor =400 pixeles\n",
    "    en la dimensión x, y'''\n",
    "    imagen = Image.open(directorio_imagen).convert('RGB')\n",
    "    # transformar las imagenes -- imagenes grandes disminuyen velocidad de procesamiento\n",
    "    if max(imagen.size) > tamaño_maximo:\n",
    "        tamaño = tamaño_maximo\n",
    "    else:\n",
    "        tamaño = max(imagen.size)\n",
    "        \n",
    "    if shape is not None:\n",
    "        tamaño = shape\n",
    "        \n",
    "    en_transform = transforms.Compose([\n",
    "                        transforms.Resize(tamaño),\n",
    "                        transforms.ToTensor()])\n",
    "    \n",
    "    # descartar el transparente canal alpha y anadir la dimensión del lote\n",
    "    imagen = en_transform(imagen)[:3,:,:].unsqueeze(0)\n",
    "    return imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "xjSGTA44HQtK35cE7sgbWR",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/content/lilyOG.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m content \u001b[39m=\u001b[39m cargar_imagen(\u001b[39m'\u001b[39;49m\u001b[39m../data/content/lilyOG.jpg\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m style \u001b[39m=\u001b[39m cargar_imagen(\u001b[39m'\u001b[39m\u001b[39m..data/style/litios_style.jpg\u001b[39m\u001b[39m'\u001b[39m, shape \u001b[39m=\u001b[39m content\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:])\u001b[39m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m, in \u001b[0;36mcargar_imagen\u001b[0;34m(directorio_imagen, tamaño_maximo, shape)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcargar_imagen\u001b[39m(directorio_imagen, tamaño_maximo \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m, shape \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m ):\n\u001b[1;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Cargar y transformar una imagen aegurando que su tamanño sea menor =400 pixeles\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m    en la dimensión x, y'''\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     imagen \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(directorio_imagen)\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[39m# transformar las imagenes -- imagenes grandes disminuyen velocidad de procesamiento\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mmax\u001b[39m(imagen\u001b[39m.\u001b[39msize) \u001b[39m>\u001b[39m tamaño_maximo:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/PIL/Image.py:3227\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3224\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[1;32m   3226\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[0;32m-> 3227\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   3228\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   3230\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/content/lilyOG.jpg'"
     ]
    }
   ],
   "source": [
    "content = cargar_imagen('../data/content/lilyOG.jpg').to(device)\n",
    "style = cargar_imagen('..data/style/litios_style.jpg', shape = content.shape[-2:]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "KsKPvAEJ1mlCVRPIgVJ3mS",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def im_convert(tensor):\n",
    "    ''' Presentar imagen como Tensor'''\n",
    "    imagen = tensor.to('cpu').clone().detach()\n",
    "    imagen = imagen.numpy().squeeze()\n",
    "    imagen = imagen.transpose(1, 2, 0)\n",
    "    #imagen = imagen * np.array((0.029, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    imagen = imagen.clip(0, 1)\n",
    "    \n",
    "    return imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "UHpx9DxW7jryuDTQOQY0tS",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Presentar las imagenes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\n",
    "# Poner imagenes de contenido y estilo lado a lado\n",
    "ax1.imshow(im_convert(content))\n",
    "ax2.imshow(im_convert(style))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "cybczfT5oFbHC0LpBr9Gps",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def obtener_carac(imagen, modelo, capas=None):\n",
    "    ''' Pasar la imagen a travez del modelo y obtener las características para un grupo de capas\n",
    "        Capas de defecto del VGNet so iguales a las Gatys et al. (2016)'''\n",
    "    # mapa de los nombres de las capas de PyTorch VGGNet \n",
    "    if capas is None:\n",
    "        capas = {'0'  : 'conv1_1',\n",
    "                 '5'  : 'conv2_1', \n",
    "                 '10' : 'conv3_1',\n",
    "                 '19' : 'conv4_1',\n",
    "                 '21' : 'conv4_2',\n",
    "                 '28' : 'conv5_1'}\n",
    "        \n",
    "    características = {}\n",
    "    x = imagen\n",
    "        # model.module es un diccionary que contiene todos los módulos en el model\n",
    "    for nombre, capa, in modelo._modules.items():\n",
    "        x = capa(x)\n",
    "        if nombre in capas:\n",
    "            características[capas[nombre]] = x\n",
    "    return características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "g4Dqz5QwQnN2nzJpBPfn3S",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def matriz_Gram(tensor):\n",
    "    ''' Calcula la Matriz Gram\n",
    "        https://es.wikipedia.org/wiki/Matriz_de_Gram\n",
    "    '''\n",
    "    # Obtener tamaño de lote (batch_size), profundidad (d),altura (h), y anchura (w) del Tensor\n",
    "    _, d, h, w = tensor.size()\n",
    "    \n",
    "    # Remodelar el Tensor para multiplicar sus características por cada canal\n",
    "    tensor = tensor.view(d, h * w)\n",
    "    \n",
    "    # Calcular la Matriz de Gram\n",
    "    gram = torch.mm(tensor, tensor.t())\n",
    "    \n",
    "    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "7Ya1jx0wC4u2X6oRcUW6Wm",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def compute_loss(vgg, content_features, style_grams, target_features, content_weight=1, style_weight=1e6):\n",
    "    \"\"\"\n",
    "    Compute the total loss for neural style transfer given the VGG model, content features, style gram matrices,\n",
    "    and target image features. \n",
    "    \n",
    "    Args:\n",
    "    - vgg: VGG model used for feature extraction\n",
    "    - content_features: Dictionary of content features extracted from the VGG model\n",
    "    - style_grams: Dictionary of style gram matrices computed from the VGG model\n",
    "    - target_features: Features of the target image\n",
    "    - content_weight: Weight of the content loss\n",
    "    - style_weight: Weight of the style loss\n",
    "    \n",
    "    Returns:\n",
    "    - Total loss value\n",
    "    \"\"\"\n",
    "    # Define the layers to use for content and style representations\n",
    "    content_layers = ['conv4_2']\n",
    "    style_layers = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1']\n",
    "    \n",
    "    # Get the target features and compute the loss for each layer\n",
    "    loss = 0\n",
    "    for layer in content_layers:\n",
    "        content_feature = content_features[layer]\n",
    "        target_feature = target_features[layer]\n",
    "        loss += content_weight * torch.mean((target_feature - content_feature)**2)\n",
    "        \n",
    "    for layer in style_layers:\n",
    "        style_gram = style_grams[layer]\n",
    "        target_feature = target_features[layer]\n",
    "        _, c, h, w = target_feature.shape\n",
    "        target_gram = matriz_Gram(target_feature)\n",
    "        layer_style_loss = style_weight * torch.mean((target_gram - style_gram)**2)\n",
    "        loss += layer_style_loss / (c * h * w)\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "p1JY433tVUZTg2BgMIEDbP",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Obtener características de contenido y estilo -- solo una vez antes de entrenar a la red\n",
    "características_contenido = obtener_carac(content, vgg)\n",
    "características_estilo    = obtener_carac(style, vgg)\n",
    "\n",
    "# Calcular la matriz de Gram para cada capa de la representación de estilo\n",
    "grams_estilo = {capa: matriz_Gram(características_estilo[capa]) for capa in características_estilo}\n",
    "\n",
    "# Crear una tercera imagen 'objetivo' y prepararla para cambios\n",
    "objetivo = content.clone().requires_grad_(True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "pVFhZJKeFEM7X9yj2E33CW",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Pesos por cada capa del estilo\n",
    "pesos_estilo = {'conv1_1' : 1.,\n",
    "              'conv2_1' : 0.75,\n",
    "              'conv3_1' : 0.2,\n",
    "              'conv4_1' : 0.2,\n",
    "              'conv5_1' : 0.2}\n",
    "\n",
    "peso_contenido = 1.0 #alpha\n",
    "#peso_estilo = 1e4  #beta\n",
    "peso_estilo = 1.0  #beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "yGCxIyJ2iRIYGB7dpLjzwZ",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "presentar_cada = 100\n",
    "\n",
    "# Iterar por los híper-parámetros\n",
    "optimizer = optim.Adam([objetivo], lr = 0.005)\n",
    "pasos = 300\n",
    "\n",
    "for ii in range(1, pasos+1):\n",
    "    # obtener características de la imagen objetivo\n",
    "    características_objetivo = obtener_carac(objetivo, vgg)\n",
    "    \n",
    "    # pérdida en contenido\n",
    "    pérdida_contenido = torch.mean((características_objetivo['conv4_2'] - características_contenido['conv4_2'])**2)\n",
    "    \n",
    "    # pérdida en estilo\n",
    "    # iniciar la pérdida en estilo en zero\n",
    "    pérdida_estilo = 0 \n",
    "    # añadir la pérdida de la matriz de Gram por cada capa\n",
    "    for capa in pesos_estilo:\n",
    "        # obtener el estilo de representración del objetivo en cada capa\n",
    "        característica_objetivo = características_objetivo[capa]\n",
    "        gram_objetivo = matriz_Gram(característica_objetivo)\n",
    "        _, d, h, w = característica_objetivo.shape\n",
    "        # Obtener el 'estilo' en la representación de estilo\n",
    "        gram_estilo = grams_estilo[capa]\n",
    "        \n",
    "        # Pérdida de estilo por capa -- añadiendo peso apropiadamente\n",
    "        pérdida_capa_estilo = pesos_estilo[capa] * torch.mean((gram_objetivo - gram_estilo)**2)\n",
    "        # Añadir perdida en estilo\n",
    "        pérdida_estilo += pérdida_capa_estilo / (d * h* w)\n",
    "        \n",
    "    # Calcular pérdida total\n",
    "    pérdida_total = peso_contenido * pérdida_contenido + peso_estilo * pérdida_estilo\n",
    "    \n",
    "    # Actualizar la imagen objetivo\n",
    "    optimizer.zero_grad()\n",
    "    pérdida_total.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Presentar las imagenes intermedias para ver como van cambiando y mostrar la pérdida\n",
    "    if ii % presentar_cada == 0:\n",
    "        print(\"Pérdida Total: \", pérdida_total.item())\n",
    "        plt.imshow(im_convert(objetivo))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "f3CyqQxHXgt5PjBgReZhYX",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize = (20, 10))\n",
    "ax1.imshow(im_convert(content))\n",
    "ax2.imshow(im_convert(style))\n",
    "ax3.imshow(im_convert(objetivo))"
   ]
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "version": 1
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
